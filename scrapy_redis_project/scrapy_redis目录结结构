
# 源码
from scrapy_redis.spiders import RedisSpider

'''
scrapy_redis/
	|-connection.py  # 根据配置文件,连接redise
	|-defaults.py  # 默认配置文件
	|-dupefilter.py  # 去重规则
	|-picklecompat.py  # 将对象经过序列化后的形式恢复到原有的对象
	|-pipelines.py  # 持久化
	|-queue.py  # 通过列表完成队列（堆、栈）
	|-scheduler.py  # 调度器 通过调用队列，列表、集合取值，处理
	|-spiders.py  # 爬虫
	|-utils.py  # 字节转字符串
'''

- scrapy-redis流程：
	- 1.引擎，获取起始url（request 对象），
	- 2.加入到调度器（pickle request对象）
		- scrapy内部有自己的调度器
		- scrapy-redis也有调度器（三种）
			- 先进先出
			- 后进先出
			- 有序集合
	- 3.调度器通知下载器可以开始下载
	- 4.下载器去调度器中获取数据
		- 有数据获取（request对象），远程无法获取对象，需要通过pickle序列化
		- 没有数据定时等待
	- 5.下载后，叫个爬虫parse方法，yield （2种）
		- item（）交给pipeline
		- request（）交给调度器处理，调度器调用DUPEFILTER_CLASS检测 是否已访问过
			- 访问过
			- 未访问 回到 2
- ps：可以扩充功能：
	- 下载中间件、爬虫中间件
	- 给予信号的扩展